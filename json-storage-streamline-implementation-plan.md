# JSON Storage Streamlining - Implementation Plan

## Executive Summary

This plan implements the consolidation from **10+ JSON files** to **4 strategic files** per run, aligning with the proposed PostgreSQL schema for future database insertion.

**Files to Keep**:
1. `ocr_filtered.json` - Filtered OCR pages (debugging)
2. `extractor_filtered.json` - Extracted fields (debugging)
3. `doc_type_check_filtered.json` - Doc type result (debugging)
4. `final.json` - **PostgreSQL-ready payload** (permanent record)

**Files to Delete**:
- `ocr_response_raw.json` (~500KB-5MB)
- `doc_type_check.raw.json`
- `extractor.raw.json`
- `merged.json`
- `manifest.json`
- `side_by_side.json`
- `metadata.json`

---

## Critical Design Decisions

### 1. Trace ID vs Run ID

**Confirmed Distinction**:
- **`trace_id`**: Generated by HTTP middleware (`exception_middleware.py:37`) for **distributed tracing** across HTTP requests/responses
  - Purpose: Correlate logs, errors, and API responses within a single HTTP transaction
  - Lifetime: Single HTTP request/response cycle
  - Generated: At middleware level using `uuid.uuid4()`
  - **Flow**: Middleware → Endpoint (`request.state.trace_id`) → Pipeline (via context) → `final.json`

- **`run_id`**: Generated by pipeline orchestrator (`orchestrator.py:96`) for **pipeline execution tracking**
  - Purpose: Unique identifier for each pipeline run and its artifacts
  - Lifetime: Entire pipeline execution + stored artifacts
  - Generated: At pipeline start using `uuid.uuid4()`

**Implementation**: Store both in `final.json` as separate fields.

---

### 2. Status Field Logic

Based on SQL schema constraints and current error handling:

#### Scenario A: System Errors (HTTP Layer Failures)
```python
# Examples: OCR_FAILED, LLM_TIMEOUT, S3_ERROR, VALIDATION_ERROR
{
    "status": "error",             # Request failed before/during pipeline
    "http_error_code": "OCR_FAILED",
    "http_error_message": "OCR service failed",
    "http_error_category": "server_error",
    "http_error_retryable": true,
    
    # All extracted_* fields: NULL (pipeline didn't complete)
    "extracted_fio": null,
    "extracted_doc_date": null,
    ...
    
    # All rule_* fields: NULL (validation didn't run)
    "rule_fio_match": null,
    "rule_doc_date_valid": null,
    "rule_verdict": null,  # ⚠️ NULL, NOT false
    "rule_errors": []
}
```

#### Scenario B: Business Rule Failures
```python
# Examples: FIO_MISMATCH, DOC_DATE_TOO_OLD
{
    "status": "success",           # Pipeline completed successfully
    "http_error_code": null,
    "http_error_message": null,
    ...
    
    # Extracted data populated
    "extracted_fio": "John Doe",
    "extracted_doc_date": "2020-01-01",
    ...
    
    # Business rules failed
    "rule_fio_match": false,
    "rule_doc_date_valid": false,
    "rule_verdict": false,         # At least one check failed
    "rule_errors": ["FIO_MISMATCH", "DOC_DATE_TOO_OLD"]
}
```

#### Scenario C: Complete Success
```python
{
    "status": "success",
    "http_error_code": null,
    ...
    
    "extracted_fio": "John Doe",
    "extracted_doc_date": "2024-01-01",
    ...
    
    "rule_fio_match": true,
    "rule_doc_date_valid": true,
    "rule_doc_type_known": true,
    "rule_single_doc_type": true,
    "rule_verdict": true,          # All checks passed
    "rule_errors": []
}
```

---

### 3. Processing Time Calculation

**Current Implementation**: Calculated at endpoint level
```python
# main.py:140, 333, 512
start_time = time.time()
# ... pipeline execution ...
processing_time = time.time() - start_time
```

**Recommendation**: ✅ **Keep current approach** - measures total API response time including S3 downloads, which is what users care about.

**Decision**: ✅ **Use pipeline-internal timing** (`ctx.artifacts["duration_seconds"]`)
- Calculated by `_finalize_timing_artifacts()` which tracks pipeline execution time
- Simpler than endpoint-level calculation (no need to update file after return)
- Accurate for pipeline performance metrics

---

## PostgreSQL Schema - Final Review

### Schema Structure

```sql
CREATE TABLE runs (
    -- ========================================
    -- PRIMARY KEYS
    -- ========================================
    id SERIAL PRIMARY KEY,
    run_id UUID NOT NULL UNIQUE DEFAULT gen_random_uuid(),
    
    -- ========================================
    -- OVERALL STATUS (HTTP LAYER)
    -- ========================================
    status VARCHAR(20) NOT NULL, 
    CONSTRAINT runs_status_check CHECK (status IN ('success', 'error')),
    
    -- ========================================
    -- HTTP ERROR DETAILS (RFC 7807)
    -- Only populated when status='error'
    -- ========================================
    http_error_category VARCHAR(20),
    CONSTRAINT runs_http_error_category_check CHECK (
        http_error_category IS NULL OR http_error_category IN ('client_error', 'server_error')
    ),
    
    http_error_code VARCHAR(100),
    http_error_message TEXT,
    http_error_retryable BOOLEAN,
    
    -- ========================================
    -- REQUEST METADATA (FROM KAFKA)
    -- ========================================
    external_request_id VARCHAR(100),
    external_s3_path VARCHAR(1024),
    external_iin VARCHAR(12),
    external_first_name VARCHAR(100) NOT NULL,
    external_last_name VARCHAR(100) NOT NULL,
    external_second_name VARCHAR(100),
    
    -- ========================================
    -- EXTRACTED DATA (FROM PIPELINE)
    -- Only populated when status='success'
    -- ========================================
    extracted_fio TEXT,
    extracted_doc_date VARCHAR(20),
    extracted_single_doc_type BOOLEAN,
    extracted_doc_type_known BOOLEAN,
    extracted_doc_type VARCHAR(200),
    
    -- ========================================
    -- BUSINESS RULE CHECKS
    -- Only populated when status='success'
    -- ========================================
    rule_fio_match BOOLEAN,
    rule_doc_date_valid BOOLEAN,
    rule_doc_type_known BOOLEAN,
    rule_single_doc_type BOOLEAN,
    
    -- ========================================
    -- FINAL RULE VERDICT & ERRORS
    -- Only populated when status='success'
    -- ========================================
    rule_verdict BOOLEAN,  -- ⚠️ Can be NULL when status='error'
    rule_errors JSONB DEFAULT '[]'::jsonb,
    
    -- ========================================
    -- TIMING
    -- ========================================
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    completed_at TIMESTAMP,
    processing_time_seconds FLOAT,
    
    -- ========================================
    -- TRACING
    -- ========================================
    trace_id UUID,
    
    -- ========================================
    -- CONSTRAINTS
    -- ========================================
    CONSTRAINT error_state_consistency CHECK (
        (status = 'error' AND rule_fio_match IS NULL)
        OR
        (status = 'success')
    ),
    
    CONSTRAINT success_state_consistency CHECK (
        (status = 'success' AND http_error_code IS NULL)
        OR
        (status = 'error' AND http_error_code IS NOT NULL)
    )
);
```

---

## Implementation Steps

### Phase 1: Create `final.json` Builder

**File**: `pipeline/utils/db_record.py` (NEW)

**Purpose**: Build PostgreSQL-ready JSON structure from pipeline context

```python
"""Build final.json structure matching PostgreSQL schema."""

from typing import Any
from pathlib import Path
from datetime import datetime


def build_final_json_for_error(
    run_id: str,
    trace_id: str | None,
    error_code: str,
    error_message: str,
    error_category: str,
    error_retryable: bool,
    external_request_id: str | None,
    external_s3_path: str | None,
    external_iin: str | None,
    external_first_name: str | None,
    external_last_name: str | None,
    external_second_name: str | None,
    created_at: str,
    completed_at: str,
    processing_time_seconds: float,
) -> dict[str, Any]:
    """Build final.json for system/HTTP errors (status='error')."""
    return {
        "run_id": run_id,
        "status": "error",
        
        # HTTP Error Details
        "http_error_category": error_category,
        "http_error_code": error_code,
        "http_error_message": error_message,
        "http_error_retryable": error_retryable,
        
        # Request Metadata
        "external_request_id": external_request_id,
        "external_s3_path": external_s3_path,
        "external_iin": external_iin,
        "external_first_name": external_first_name,
        "external_last_name": external_last_name,
        "external_second_name": external_second_name,
        
        # Extracted Data: NULL (pipeline didn't complete)
        "extracted_fio": None,
        "extracted_doc_date": None,
        "extracted_single_doc_type": None,
        "extracted_doc_type_known": None,
        "extracted_doc_type": None,
        
        # Rule Checks: NULL (validation didn't run)
        "rule_fio_match": None,
        "rule_doc_date_valid": None,
        "rule_doc_type_known": None,
        "rule_single_doc_type": None,
        
        # Verdict & Errors
        "rule_verdict": None,  # NULL for errors
        "rule_errors": [],
        
        # Timing
        "created_at": created_at,
        "completed_at": completed_at,
        "processing_time_seconds": processing_time_seconds,
        
        # Tracing
        "trace_id": trace_id,
    }


def build_final_json_for_success(
    run_id: str,
    trace_id: str | None,
    external_request_id: str | None,
    external_s3_path: str | None,
    external_iin: str | None,
    external_first_name: str | None,
    external_last_name: str | None,
    external_second_name: str | None,
    extracted_fio: str | None,
    extracted_doc_date: str | None,
    extracted_single_doc_type: bool | None,
    extracted_doc_type_known: bool | None,
    extracted_doc_type: str | None,
    rule_fio_match: bool | None,
    rule_doc_date_valid: bool | None,
    rule_doc_type_known: bool | None,
    rule_single_doc_type: bool | None,
    rule_verdict: bool,
    rule_errors: list[str],
    created_at: str,
    completed_at: str,
    processing_time_seconds: float,
) -> dict[str, Any]:
    """Build final.json for successful pipeline runs (status='success')."""
    return {
        "run_id": run_id,
        "status": "success",
        
        # HTTP Error Details: NULL (no HTTP error)
        "http_error_category": None,
        "http_error_code": None,
        "http_error_message": None,
        "http_error_retryable": None,
        
        # Request Metadata
        "external_request_id": external_request_id,
        "external_s3_path": external_s3_path,
        "external_iin": external_iin,
        "external_first_name": external_first_name,
        "external_last_name": external_last_name,
        "external_second_name": external_second_name,
        
        # Extracted Data
        "extracted_fio": extracted_fio,
        "extracted_doc_date": extracted_doc_date,
        "extracted_single_doc_type": extracted_single_doc_type,
        "extracted_doc_type_known": extracted_doc_type_known,
        "extracted_doc_type": extracted_doc_type,
        
        # Rule Checks
        "rule_fio_match": rule_fio_match,
        "rule_doc_date_valid": rule_doc_date_valid,
        "rule_doc_type_known": rule_doc_type_known,
        "rule_single_doc_type": rule_single_doc_type,
        
        # Verdict & Errors
        "rule_verdict": rule_verdict,
        "rule_errors": rule_errors,  # e.g., ["FIO_MISMATCH", "DOC_DATE_TOO_OLD"]
        
        # Timing
        "created_at": created_at,
        "completed_at": completed_at,
        "processing_time_seconds": processing_time_seconds,
        
        # Tracing
        "trace_id": trace_id,
    }
```

---

### Phase 2: Modify Pipeline Orchestrator

**File**: `pipeline/orchestrator.py`

#### 2.1 Update PipelineContext

```python
@dataclass
class PipelineContext:
    # ... existing fields ...
    
    # NEW: Trace ID (generated by middleware, passed from endpoint)
    trace_id: str | None = None
    
    # NEW: Kafka/external metadata (from request)
    external_request_id: str | None = None
    external_s3_path: str | None = None
    external_iin: str | None = None
    external_first_name: str | None = None
    external_last_name: str | None = None
    external_second_name: str | None = None
```

#### 2.2 Update `fail_and_finalize()`

Stop writing:
- ❌ `manifest.json`
- ❌ `side_by_side.json`

Start writing:
- ✅ `final.json` (PostgreSQL format)

```python
def fail_and_finalize(code: str, details: str | None, ctx: PipelineContext) -> dict[str, Any]:
    """Finalize pipeline with system error (status='error')."""
    from pipeline.utils.db_record import build_final_json_for_error
    
    ctx.errors.append(make_error(code, details=details))
    _finalize_timing_artifacts(ctx)  # Calculates ctx.artifacts["duration_seconds"]
    
    # Determine error category and retryability
    error_category, error_retryable = _classify_error(code)
    
    # Use pipeline-internal timing
    processing_time = ctx.artifacts.get("duration_seconds", 0.0)
    completed_at = datetime.now(timezone(timedelta(hours=UTC_OFFSET_HOURS))).isoformat()
    
    final_json = build_final_json_for_error(
        run_id=ctx.run_id,
        trace_id=ctx.trace_id,
        error_code=code,
        error_message=_get_error_message(code),
        error_category=error_category,
        error_retryable=error_retryable,
        external_request_id=ctx.external_request_id,
        external_s3_path=ctx.external_s3_path,
        external_iin=ctx.external_iin,
        external_first_name=ctx.external_first_name,
        external_last_name=ctx.external_last_name,
        external_second_name=ctx.external_second_name,
        created_at=ctx.request_created_at,
        completed_at=completed_at,
        processing_time_seconds=processing_time,  # ← From pipeline timing
    )
    
    # Write final.json
    final_path = ctx.meta_dir / "final.json"
    util_write_json(final_path, final_json)
    ctx.artifacts["final_result_path"] = str(final_path)
    
    # Return for API response (existing format)
    return {
        "run_id": ctx.run_id,
        "verdict": False,
        "errors": ctx.errors,
    }


def _classify_error(code: str) -> tuple[str, bool]:
    """Classify error into category and retryability."""
    # Server errors (retryable)
    if code in ("OCR_FAILED", "LLM_TIMEOUT", "S3_ERROR", "DTC_FAILED", "EXTRACT_FAILED"):
        return "server_error", True
    
    # Client errors (not retryable)
    if code in ("PDF_TOO_MANY_PAGES", "FILE_SAVE_FAILED", "VALIDATION_ERROR"):
        return "client_error", False
    
    # Default: server error, not retryable
    return "server_error", False


def _get_error_message(code: str) -> str:
    """Get human-readable error message."""
    messages = {
        "OCR_FAILED": "OCR service failed",
        "LLM_TIMEOUT": "LLM service timeout",
        "S3_ERROR": "S3 service error",
        "PDF_TOO_MANY_PAGES": "PDF has too many pages",
        "MULTIPLE_DOCUMENTS": "Multiple document types detected",
        # ... add all error codes
    }
    return messages.get(code, f"Error: {code}")
```

#### 2.3 Update `finalize_success()`

```python
def finalize_success(verdict: bool, checks: dict[str, Any] | None, ctx: PipelineContext) -> dict[str, Any]:
    """Finalize pipeline with successful completion (status='success')."""
    from pipeline.utils.db_record import build_final_json_for_success
    
    _finalize_timing_artifacts(ctx)  # Calculates ctx.artifacts["duration_seconds"]
    
    # Read extracted data
    extractor_data = _read_extractor_data(ctx)
    doc_type_data = _read_doc_type_data(ctx)
    
    # Build rule_errors list
    rule_errors = [error["code"] for error in ctx.errors]
    
    # Use pipeline-internal timing
    processing_time = ctx.artifacts.get("duration_seconds", 0.0)
    completed_at = datetime.now(timezone(timedelta(hours=UTC_OFFSET_HOURS))).isoformat()
    
    final_json = build_final_json_for_success(
        run_id=ctx.run_id,
        trace_id=ctx.trace_id,
        external_request_id=ctx.external_request_id,
        external_s3_path=ctx.external_s3_path,
        external_iin=ctx.external_iin,
        external_first_name=ctx.external_first_name,
        external_last_name=ctx.external_last_name,
        external_second_name=ctx.external_second_name,
        extracted_fio=extractor_data.get("fio"),
        extracted_doc_date=extractor_data.get("doc_date"),
        extracted_single_doc_type=doc_type_data.get("single_doc_type"),
        extracted_doc_type_known=doc_type_data.get("doc_type_known"),
        extracted_doc_type=_format_doc_type(doc_type_data.get("detected_doc_types")),
        rule_fio_match=checks.get("fio_match") if checks else None,
        rule_doc_date_valid=checks.get("doc_date_valid") if checks else None,
        rule_doc_type_known=checks.get("doc_type_known") if checks else None,
        rule_single_doc_type=doc_type_data.get("single_doc_type"),
        rule_verdict=verdict,
        rule_errors=rule_errors,
        created_at=ctx.request_created_at,
        completed_at=completed_at,
        processing_time_seconds=processing_time,  # ← From pipeline timing
    )
    
    # Write final.json
    final_path = ctx.meta_dir / "final.json"
    util_write_json(final_path, final_json)
    ctx.artifacts["final_result_path"] = str(final_path)
    
    # Return for API response (existing format)
    return {
        "run_id": ctx.run_id,
        "verdict": verdict,
        "errors": ctx.errors,
    }


def _read_extractor_data(ctx: PipelineContext) -> dict:
    """Read extractor filtered data."""
    try:
        path = ctx.artifacts.get("llm_extractor_filtered_path")
        if path:
            return util_read_json(path)
    except Exception:
        pass
    return {}


def _read_doc_type_data(ctx: PipelineContext) -> dict:
    """Read doc type filtered data."""
    try:
        path = ctx.artifacts.get("llm_doc_type_check_filtered_path")
        if path:
            return util_read_json(path)
    except Exception:
        pass
    return {}


def _format_doc_type(detected_types: list | None) -> str | None:
    """Convert list of detected types to single string."""
    if not detected_types:
        return None
    if isinstance(detected_types, list) and len(detected_types) > 0:
        return detected_types[0]  # Take first detected type
    return None
```

#### 2.4 Refactor Validator to Eliminate File Dependencies

**File**: `pipeline/processors/validator.py`

Refactor `validate_run()` to accept data directly instead of file paths:

```python
def validate_run(
    user_provided_fio: str | None,  # NEW: Pass directly from context
    extractor_filtered_path: str,   # Changed from merged_path
    doc_type_filtered_path: str,    # NEW: Read doc type data separately
    output_dir: str,
    filename: str = VALIDATION_FILENAME,
    write_file: bool = True,
) -> dict[str, Any]:
    """
    Validate a single run and optionally write validation.json.
    
    Args:
      user_provided_fio: FIO from user/Kafka (passed via PipelineContext)
      extractor_filtered_path: Path to extractor.filtered.json
      doc_type_filtered_path: Path to doc_type_check.filtered.json
      output_dir: Directory where validation.json should be written
      filename: Validation filename; defaults to VALIDATION_FILENAME
      write_file: If True, write validation.json; otherwise return only in-memory result
    """
    # Read extractor data
    try:
        with open(extractor_filtered_path, encoding="utf-8") as ef:
            extractor_data = json.load(ef)
    except Exception as e:
        return {"success": False, "error": f"IO error reading extractor: {e}", "validation_path": "", "result": None}
    
    # Read doc type data
    try:
        with open(doc_type_filtered_path, encoding="utf-8") as df:
            doc_type_data = json.load(df)
    except Exception as e:
        return {"success": False, "error": f"IO error reading doc_type: {e}", "validation_path": "", "result": None}
    
    # Build merged data in-memory (don't write to file)
    merged = {
        "fio": extractor_data.get("fio"),
        "doc_date": extractor_data.get("doc_date"),
        "single_doc_type": doc_type_data.get("single_doc_type"),
        "doc_type_known": doc_type_data.get("doc_type_known"),
        "doc_type": doc_type_data.get("detected_doc_types", [None])[0] if isinstance(doc_type_data.get("detected_doc_types"), list) else None,
    }
    
    # Use passed FIO instead of reading from metadata.json
    fio_meta_raw = user_provided_fio
    
    # Rest of validation logic remains the same...
    # (FIO matching, doc type validation, date validation, etc.)
```

#### 2.5 Update `stage_validate_and_finalize()` 

**File**: `pipeline/orchestrator.py`

Update to pass FIO and filtered file paths directly:

```python
def stage_validate_and_finalize(ctx: PipelineContext) -> dict[str, Any] | None:
    try:
        with stage_timer(ctx, "llm"):
            validation = validate_run(
                user_provided_fio=ctx.fio,  # ← Pass from context instead of file
                extractor_filtered_path=ctx.artifacts.get("llm_extractor_filtered_path", ""),
                doc_type_filtered_path=ctx.artifacts.get("llm_doc_type_check_filtered_path", ""),
                output_dir=str(ctx.validation_dir),
                filename=VALIDATION_FILENAME,
                write_file=False,
            )
        # ... rest of validation logic ...
```

#### 2.6 Remove File Writes

Delete these lines from `pipeline/orchestrator.py`:

```python
# ❌ REMOVE from stage_acquire() (line 245-246):
metadata = {"fio": ctx.fio or None}
util_write_json(ctx.meta_dir / METADATA_FILENAME, metadata)

# ❌ REMOVE from stage_merge() (line 348-353):
merged_path = merge_extractor_and_doc_type(
    extractor_filtered_path=...,
    doc_type_filtered_path=...,
    output_dir=str(ctx.llm_dir),
    filename=MERGED_FILENAME,  # Don't write merged.json
)
ctx.artifacts["llm_merged_path"] = str(merged_path)

# ❌ REMOVE from stage_merge() (line 355-362):
try:
    util_build_side_by_side(
        meta_dir=ctx.meta_dir,
        merged_path=merged_path,
        request_created_at=ctx.request_created_at,
    )
except Exception:
    pass

# ❌ REMOVE from fail_and_finalize() and finalize_success():
util_write_manifest(...)  # No longer writing manifests
util_build_final_result(...)  # Replaced by new final.json format
```

**Note**: `stage_merge()` can be completely removed from the pipeline since we're reading filtered files directly in the validator.

---

### Phase 3: Stop Writing Raw JSONs

**File**: `pipeline/clients/tesseract_async_client.py`

```python
# Line 257: Change save_json=True to save_json=False
ocr_result = ask_tesseract(
    str(ctx.saved_path), 
    output_dir=str(ctx.ocr_dir), 
    save_json=False  # ❌ Don't write ocr_response_raw.json
)
```

**File**: `pipeline/orchestrator.py`

In `stage_doc_type_check()` and `stage_extract()`:

```python
# ❌ REMOVE: Writing raw LLM responses
# DELETE THESE LINES:
dtc_raw_path = ctx.llm_dir / LLM_DOC_TYPE_RAW
with open(dtc_raw_path, "w", encoding="utf-8") as f:
    f.write(dtc_raw_str or "")

# ❌ REMOVE: Writing extractor raw
llm_raw_path = ctx.llm_dir / LLM_EXTRACTOR_RAW
with open(llm_raw_path, "w", encoding="utf-8") as f:
    f.write(llm_raw or "")
```

**Instead**: Process LLM responses in-memory, write only filtered versions.

**Note**: `stage_merge()` can be completely removed from the pipeline orchestrator since validation now reads filtered files directly.

---

### Phase 4: Update Endpoint Integration

**File**: `main.py`

Pass `trace_id` (from middleware) and Kafka metadata to pipeline:

```python
@app.post("/v1/kafka/verify")
async def verify_kafka_event(request: Request, event: KafkaEventRequest):
    start_time = time.time()
    
    # Get trace_id from middleware (already generated by exception_middleware)
    trace_id = getattr(request.state, "trace_id", None)
    
    # Build external metadata to pass to pipeline
    external_data = {
        "trace_id": trace_id,  # From middleware, for storage in final.json
        "external_request_id": str(event.request_id),
        "external_s3_path": event.s3_path,
        "external_iin": str(event.iin),
        "external_first_name": event.first_name,
        "external_last_name": event.last_name,
        "external_second_name": event.second_name,
    }
    
    # Pass to processor
    result = await processor.process_kafka_event(
        event_data=event.dict(),
        external_metadata=external_data,  # NEW
    )
    
    # Note: processing_time_seconds is already in final.json
    # (calculated internally by pipeline using _finalize_timing_artifacts)
    
    return VerifyResponse(...)
```

**File**: `services/processor.py`

```python
async def process_kafka_event(
    self,
    event_data: dict,
    external_metadata: dict | None = None,  # NEW
) -> dict:
    # ... existing S3 download logic ...
    
    # Run pipeline with external metadata
    result = run_pipeline(
        fio=fio,
        source_file_path=str(local_file_path),
        original_filename=filename,
        content_type="application/pdf",
        runs_root=self.runs_root,
        external_metadata=external_metadata,  # NEW
    )
    
    return result
```

**File**: `pipeline/orchestrator.py`

```python
def run_pipeline(
    fio: str | None,
    source_file_path: str,
    original_filename: str,
    content_type: str | None,
    runs_root: Path,
    external_metadata: dict | None = None,  # NEW: Contains trace_id and Kafka fields
) -> dict[str, Any]:
    run_id = _generate_run_id()  # Pipeline-level ID (separate from trace_id)
    request_created_at = datetime.now(...).isoformat()
    dirs = _mk_run_dirs(runs_root, run_id)
    
    # Extract external metadata (trace_id from middleware, Kafka fields from request)
    ext_meta = external_metadata or {}
    
    ctx = PipelineContext(
        fio=fio,
        source_file_path=source_file_path,
        original_filename=original_filename,
        content_type=content_type,
        runs_root=runs_root,
        run_id=run_id,
        request_created_at=request_created_at,
        dirs=dirs,
        # NEW: trace_id from middleware (passed through endpoint)
        trace_id=ext_meta.get("trace_id"),
        # NEW: Kafka/external metadata from request
        external_request_id=ext_meta.get("external_request_id"),
        external_s3_path=ext_meta.get("external_s3_path"),
        external_iin=ext_meta.get("external_iin"),
        external_first_name=ext_meta.get("external_first_name"),
        external_last_name=ext_meta.get("external_last_name"),
        external_second_name=ext_meta.get("external_second_name"),
    )
    
    # ... rest of pipeline ...
```

---

### Phase 5: Rename Filtered Files

Update filename constants:

**File**: `pipeline/core/config.py`

```python
# Before:
OCR_PAGES = "ocr_response_filtered.json"
LLM_DOC_TYPE_FILTERED = "doc_type_check.filtered.json"
LLM_EXTRACTOR_FILTERED = "extractor.filtered.json"

# After:
OCR_FILTERED = "ocr_filtered.json"
DOC_TYPE_FILTERED = "doc_type_check_filtered.json"
EXTRACTOR_FILTERED = "extractor_filtered.json"

# DELETE (no longer written):
# OCR_RAW = "ocr_response_raw.json"
# LLM_DOC_TYPE_RAW = "doc_type_check.raw.json"
# LLM_EXTRACTOR_RAW = "extractor.raw.json"
# MERGED_FILENAME = "merged.json"         # ← Eliminated (validator reads filtered files directly)
# METADATA_FILENAME = "metadata.json"     # ← Eliminated (FIO passed via context)
# VALIDATION_FILENAME = "validation.json" # ← Not written to disk (write_file=False)
```

Update all references throughout codebase.

---

### Phase 6: Delete Old Artifact Functions

**File**: `pipeline/utils/artifacts.py`

Delete these functions (no longer needed):
```python
# ❌ DELETE:
def build_final_result(...)  # Replaced by build_final_json_for_success/error
def write_manifest(...)       # No longer writing manifests
def build_side_by_side(...)   # No longer writing side_by_side
```

---

## Directory Structure - Before vs After

### Before (10+ files)
```
runs/YYYY-MM-DD/{run_id}/
├── input/original/
│   └── document.pdf
├── ocr/
│   ├── ocr_response_raw.json         # ❌ DELETE (~5MB)
│   └── ocr_response_filtered.json    # ✅ RENAME
├── llm/
│   ├── doc_type_check.raw.json       # ❌ DELETE
│   ├── doc_type_check.filtered.json  # ✅ RENAME
│   ├── extractor.raw.json            # ❌ DELETE
│   ├── extractor.filtered.json       # ✅ RENAME
│   └── merged.json                   # ❌ DELETE
└── meta/
    ├── metadata.json                 # ❌ DELETE
    ├── manifest.json                 # ❌ DELETE
    ├── side_by_side.json             # ❌ DELETE
    └── final_result.json             # ❌ REPLACE
```

### After (4 files) ✅
```
runs/YYYY-MM-DD/{run_id}/
├── input/original/
│   └── document.pdf
├── ocr/
│   └── ocr_filtered.json             # ✅ KEEP (debugging)
├── llm/
│   ├── doc_type_check_filtered.json  # ✅ KEEP (debugging)
│   └── extractor_filtered.json       # ✅ KEEP (debugging)
└── meta/
    └── final.json                    # ✅ NEW (PostgreSQL-ready)
```

**Key Changes**:
- ❌ **Eliminated**: `merged.json` (validator reads filtered files directly)
- ❌ **Eliminated**: `metadata.json` (FIO passed via `PipelineContext`)
- ❌ **Eliminated**: `validation.json` (not written to disk, only in-memory)
- ✅ **Achieved**: Original 4-file goal!

**Storage Savings**: ~80-95% reduction per run

**Files Deleted Per Run**:
- `ocr_response_raw.json` (~5MB) - 95% of savings
- `doc_type_check.raw.json` (~20KB)
- `extractor.raw.json` (~20KB)
- `merged.json` (~2KB) - eliminated by refactoring
- `metadata.json` (~0.5KB) - eliminated by passing FIO via context
- `manifest.json` (~3KB)
- `side_by_side.json` (~2KB)
- `final_result.json` (old format) (~1KB)

---

## Testing Strategy

### Unit Tests

**File**: `tests/test_db_record.py` (NEW)

```python
def test_build_final_json_for_error():
    """Test error scenario final.json structure."""
    result = build_final_json_for_error(
        run_id="test-123",
        trace_id="trace-456",
        error_code="OCR_FAILED",
        error_message="OCR service failed",
        error_category="server_error",
        error_retryable=True,
        external_request_id="kafka-789",
        # ... all fields ...
    )
    
    assert result["status"] == "error"
    assert result["http_error_code"] == "OCR_FAILED"
    assert result["extracted_fio"] is None
    assert result["rule_verdict"] is None  # NULL for errors
    assert result["rule_errors"] == []


def test_build_final_json_for_success():
    """Test success scenario final.json structure."""
    result = build_final_json_for_success(
        run_id="test-123",
        trace_id="trace-456",
        rule_verdict=False,
        rule_errors=["FIO_MISMATCH", "DOC_DATE_TOO_OLD"],
        # ... all fields ...
    )
    
    assert result["status"] == "success"
    assert result["http_error_code"] is None
    assert result["rule_verdict"] is False
    assert result["rule_errors"] == ["FIO_MISMATCH", "DOC_DATE_TOO_OLD"]
```

### Integration Tests

1. **Test OCR Failure** → Verify `final.json` has `status='error'`, all `rule_*` fields are NULL
2. **Test FIO Mismatch** → Verify `final.json` has `status='success'`, `rule_verdict=false`, `rule_errors=["FIO_MISMATCH"]`
3. **Test Complete Success** → Verify all fields populated correctly
4. **Test File Count** → Assert only 4 JSONs created per run

---

## Rollback Plan

If issues arise:

1. **Immediate Rollback**: Revert all changes via Git
2. **Data Recovery**: Old runs are untouched (only new runs affected)
3. **Compatibility**: No breaking changes to API responses

---

## Deployment Checklist

- [ ] Create `pipeline/utils/db_record.py`
- [ ] Update `pipeline/orchestrator.py`:
  - [ ] Add external metadata fields to `PipelineContext`
  - [ ] Rewrite `fail_and_finalize()` to use new `final.json` format
  - [ ] Rewrite `finalize_success()` to use new `final.json` format
  - [ ] Update `stage_validate_and_finalize()` to pass FIO and filtered paths
  - [ ] Remove `metadata.json` write from `stage_acquire()`
  - [ ] Remove `stage_merge()` completely (no longer needed)
  - [ ] Remove manifest/side_by_side writes
- [ ] Update `pipeline/core/config.py`:
  - [ ] Rename filename constants
  - [ ] Delete unused constants
- [ ] Update `pipeline/clients/tesseract_async_client.py`:
  - [ ] Set `save_json=False` to stop writing raw OCR
- [ ] Update `pipeline/processors/validator.py`:
  - [ ] Refactor `validate_run()` to accept `user_provided_fio` parameter
  - [ ] Change to read `extractor_filtered_path` and `doc_type_filtered_path` separately
  - [ ] Build merged data in-memory (don't write to file)
- [ ] Update `pipeline/orchestrator.py`:
  - [ ] Remove raw LLM JSON writes in `stage_doc_type_check()`
  - [ ] Remove raw LLM JSON writes in `stage_extract()`
  - [ ] Remove `stage_merge()` from pipeline stages
- [ ] Delete `pipeline/utils/artifacts.py` functions:
  - [ ] Remove `build_final_result()`
  - [ ] Remove `write_manifest()`
  - [ ] Remove `build_side_by_side()`
- [ ] Update `main.py`:
  - [ ] Pass `trace_id` and external metadata to pipeline
- [ ] Update `services/processor.py`:
  - [ ] Accept and pass external metadata
- [ ] Write unit tests in `tests/test_db_record.py`
- [ ] Run integration tests
- [ ] Verify only 4 JSONs created per run
- [ ] Deploy to staging
- [ ] Monitor for 24 hours
- [ ] Deploy to production

---

## Estimated Effort

- **Implementation**: 4-6 hours
- **Testing**: 2-3 hours
- **Deployment**: 1 hour
- **Total**: 1 working day

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Missing field in `final.json` | Low | Medium | Comprehensive unit tests |
| Incorrect `status` logic | Low | High | Clear scenario tests |
| Breaking API responses | Very Low | High | API format unchanged |
| Storage issues | Very Low | Low | Reduces storage by 80% |

---

## Success Criteria

✅ Only 4 JSON files created per run  
✅ `final.json` matches PostgreSQL schema exactly  
✅ All scenarios (error/success/failure) handled correctly  
✅ API responses unchanged  
✅ All tests pass  
✅ 80% storage reduction achieved  

---

**Document Version**: 1.0  
**Created**: 2025-12-08  
**Author**: Backend Engineering Team  
**Status**: Ready for Implementation
